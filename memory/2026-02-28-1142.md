# Session: 2026-02-28 11:42:00 UTC

- **Session Key**: agent:main:main
- **Session ID**: n/a
- **Source**: craw-hot 技能增量写入优化

## 问题

之前的 craw-hot 技能在抓取所有用户后才写入文件，如果抓取过程中出现故障或程序被中断，已经抓取的数据会丢失。

**风险场景：**
- 抓取到第 15 个用户时程序崩溃
- 已经抓取的 14 个用户的数据全部丢失
- 需要重新开始整个抓取过程

## 解决方案

**craw-hot 技能 v2.3 - 增量写入优化**

### 核心改进

1. **初始化文件**：在抓取开始时就创建文件并写入标题
2. **增量追加**：每抓取完一个用户就立即追加到文件
3. **立即写入磁盘**：使用 `flush()` 确保数据立即写入磁盘
4. **进度标注**：文件中显示当前进度 [X/Y]

### 代码修改

#### 1. 修改 `crawl_all_users` 方法

```python
def crawl_all_users(self) -> Dict[str, List[str]]:
    """抓取所有用户的当天帖子（增量保存）"""
    users = self.load_users()
    if not users:
        self.logger.warning("No users to crawl")
        return {}

    all_results = {}
    timestamp = datetime.now()

    # 创建文件并写入标题（增量模式）
    filename = f"posts_{timestamp.strftime('%Y%m%d_%H%M%S')}.txt"
    filepath = self.results_dir / filename

    # 初始化文件，写入标题
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# Crawl Results - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total users: {len(users)}\n")
        f.write(f"# Started at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.flush()  # 立即写入磁盘

    for i, user in enumerate(users, 1):
        self.logger.info(f"Progress: {i}/{len(users)} users")

        try:
            urls = self.crawl_user_posts(user)
            all_results[user] = urls  # 即使为空也保存

            # 立即追加到文件
            self._append_user_results(filepath, user, urls, i, len(users))

            if urls:
                self.logger.info(f"@{user}: {len(urls)} posts")
            else:
                self.logger.warning(f"@{user}: no posts found in the last 24 hours")
        except Exception as e:
            self.logger.error(f"Failed to crawl @{user}: {str(e)}")
            all_results[user] = []  # 失败也保存空列表

            # 即使失败也追加空结果
            self._append_user_results(filepath, user, [], i, len(users))

        # 用户之间的随机延迟，避免请求过于频繁
        if i < len(users):
            self._wait_random(2, 5)

    # 写入完成时间
    with open(filepath, "a", encoding="utf-8") as f:
        completed_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        f.write(f"\n# Completed at: {completed_time}\n")
        total_posts = sum(len(urls) for urls in all_results.values())
        f.write(f"# Total posts: {total_posts}\n")
        f.flush()  # 立即写入磁盘

    # 生成 Markdown 文件（在所有抓取完成后）
    self._save_results(all_results, timestamp)

    return all_results
```

#### 2. 新增 `_append_user_results` 方法

```python
def _append_user_results(self, filepath: Path, username: str, urls: List[str],
                       current: int, total: int) -> None:
    """增量追加单个用户的抓取结果到文件"""
    try:
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(f"# @{username} ({len(urls)} posts) - [{current}/{total}]\n")
            for url in urls:
                f.write(f"{url}\n")
            f.write("\n")
            f.flush()  # 立即写入磁盘，确保数据不丢失
    except Exception as e:
        self.logger.error(f"Failed to append results for @{username}: {str(e)}")
```

## 测试结果

**文件示例（增量写入）：**
```
# Crawl Results - 2026-02-28 11:40:02
# Total users: 19
# Started at: 2026-02-28 11:40:02

# @vista8 (14 posts) - [1/19]
https://x.com/vista8/status/2027575069980168667
https://x.com/vista8/status/2027575143737049339
...

# @karpathy (2 posts) - [2/19]
https://x.com/karpathy/status/2027521323275325622
https://x.com/karpathy/status/2027501331125239822

# @EHuanglu (4 posts) - [3/19]
https://x.com/EHuanglu/status/2027486181479297512
...
```

## 优势

✅ **数据安全**：每抓取一个用户就立即保存，故障不会丢失数据
✅ **实时监控**：可以通过文件实时查看抓取进度
✅ **断点续传**：即使程序中断，可以从已保存的数据继续
✅ **进度标注**：清晰显示当前进度 [X/Y]

## 对比

| 功能 | v2.2 (之前) | v2.3 (现在) |
|------|-------------|-------------|
| 写入时机 | 所有用户完成后 | 每完成一个用户 |
| 数据安全 | 可能丢失所有数据 | 已抓取的数据安全 |
| 进度监控 | 只能在日志中 | 可在文件中查看 |
| 断点续传 | 不支持 | 自动支持 |

## 当前状态

增量写入功能已测试验证，正在正常工作中。
