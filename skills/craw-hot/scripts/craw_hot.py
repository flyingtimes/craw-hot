#!/usr/bin/env python3
"""
Crawl Hot - X/Twitter å¸–å­æŠ“å–å·¥å…·ï¼ˆé‡æ„ç‰ˆï¼‰
ç®¡ç†å…³æ³¨ç”¨æˆ·åˆ—è¡¨å¹¶æŠ“å–å½“å¤©æœ€æ–°å¸–å­ URL

æ¶æ„ï¼š
    - æ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£æ¸…æ™°
    - DRY åŸåˆ™ï¼Œæ¶ˆé™¤é‡å¤ä»£ç 
    - é…ç½®åŒ–ï¼Œæ˜“äºç»´æŠ¤
    - ç±»å‹æç¤ºå®Œæ•´
"""

from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Callable
from datetime import datetime
from pathlib import Path
import json
import os
import sys
import random
import time
import subprocess
import requests
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed


# ============================================================================
# é…ç½®æ¨¡å—
# ============================================================================

@dataclass
class Config:
    """å…¨å±€é…ç½®"""
    # è·¯å¾„é…ç½®
    base_dir: Path = Path(__file__).parent.parent
    log_file: Path = None
    users_file: Path = None
    results_dir: Path = None

    # è¶…æ—¶é…ç½®ï¼ˆç§’ï¼‰
    page_load_timeout: int = 5
    scroll_check_interval: float = 0.5
    scroll_max_attempts: int = 10
    scroll_no_new_threshold: int = 2  # è¿ç»­ 2 æ¬¡æ²¡æ–°å¸–å°±åœæ­¢ï¼ˆä¼˜åŒ–ï¼šä» 3 æ¬¡é™ä¸º 2 æ¬¡ï¼‰
    scroll_max_consecutive_errors: int = 3
    scroll_early_stop_on_yesterday: bool = True  # æ£€æµ‹åˆ°æ˜¨å¤©çš„å¸–å­å°±ç«‹å³åœæ­¢
    user_crawl_timeout: int = 120  # 2 åˆ†é’Ÿ

    # é‡è¯•é…ç½®
    max_retries: int = 5
    browser_action_max_attempts: int = 2

    # å¹¶å‘é…ç½®
    content_fetch_workers: int = 10
    user_crawl_workers: int = 5  # ç”¨æˆ·æŠ“å–å¹¶å‘æ•°

    # æµè§ˆå™¨é…ç½®
    browser_max_restarts: int = 10
    browser_restart_backoff: int = 30

    # è¯·æ±‚å¤´
    request_headers: Dict[str, str] = None

    def __post_init__(self):
        """åˆå§‹åŒ–è·¯å¾„"""
        self.log_file = self.base_dir / "craw_hot.log"
        self.users_file = self.base_dir / "users.txt"
        self.results_dir = self.base_dir / "results"
        self.request_headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }


# ============================================================================
# å¼‚å¸¸æ¨¡å—
# ============================================================================

class NoPostsFoundError(Exception):
    """æ²¡æœ‰æ‰¾åˆ°å¸–å­çš„å¼‚å¸¸ï¼ˆç”¨äºåŒºåˆ†çœŸæ­£çš„å¤±è´¥å’Œæ­£å¸¸æƒ…å†µï¼‰"""
    pass


class BrowserActionError(Exception):
    """æµè§ˆå™¨æ“ä½œå¼‚å¸¸"""
    pass


class CrawlTimeoutError(Exception):
    """æŠ“å–è¶…æ—¶å¼‚å¸¸"""
    pass


# ============================================================================
# æ—¥å¿—æ¨¡å—
# ============================================================================

class Logger:
    """æ—¥å¿—å·¥å…·"""

    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

    def log(self, level: str, message: str) -> None:
        """å†™å…¥æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_line = f"[{timestamp}] [{level}] {message}\n"
        print(log_line.strip())
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_line)

    def info(self, message: str) -> None:
        self.log("INFO", message)

    def warning(self, message: str) -> None:
        self.log("WARN", message)

    def error(self, message: str) -> None:
        self.log("ERROR", message)

    def debug(self, message: str) -> None:
        self.log("DEBUG", message)


# ============================================================================
# è¿›ç¨‹é”æ¨¡å—
# ============================================================================

class ProcessLock:
    """è¿›ç¨‹é”ï¼Œé˜²æ­¢å¤šä¸ªå®ä¾‹åŒæ—¶è¿è¡Œ"""

    def __init__(self, lock_file: Path):
        self.lock_file = lock_file
        self.lock_fd = None

    def acquire(self) -> bool:
        """å°è¯•è·å–é”"""
        try:
            import fcntl
            self.lock_fd = open(self.lock_file, 'w')
            fcntl.flock(self.lock_fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            self.lock_fd.write(str(os.getpid()))
            self.lock_fd.flush()
            return True
        except (IOError, BlockingIOError):
            return False

    def release(self) -> None:
        """é‡Šæ”¾é”"""
        if self.lock_fd:
            import fcntl
            fcntl.flock(self.lock_fd.fileno(), fcntl.LOCK_UN)
            self.lock_fd.close()
            self.lock_fd = None

        if self.lock_file.exists():
            try:
                self.lock_file.unlink()
            except:
                pass

    def __enter__(self):
        return self.acquire()

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()


# ============================================================================
# æµè§ˆå™¨å®¢æˆ·ç«¯æ¨¡å—
# ============================================================================

class BrowserClient:
    """æµè§ˆå™¨æ“ä½œå®¢æˆ·ç«¯ï¼ˆå°è£… subprocess è°ƒç”¨ï¼‰"""

    def __init__(self, config: Config, logger: Logger):
        self.config = config
        self.logger = logger
        self.restart_count = 0

    def _run_command(self, args: List[str], timeout: int = 30) -> subprocess.CompletedProcess:
        """è¿è¡Œå‘½ä»¤"""
        return subprocess.run(
            args,
            capture_output=True,
            text=True,
            timeout=timeout
        )

    def _parse_output(self, output: str) -> Optional[Dict]:
        """è§£ææµè§ˆå™¨å‘½ä»¤è¾“å‡ºï¼ˆæ”¯æŒ JSONã€å¸ƒå°”å€¼ï¼‰"""
        lines = output.strip().split('\n')

        # ä»å‰å¾€åæ‰¾ JSON å¼€å§‹ï¼ˆ{, [, "ï¼‰
        json_start = None
        for i, line in enumerate(lines):
            line_stripped = line.strip()

            # è·³è¿‡è°ƒè¯•ä¿¡æ¯ï¼ˆä»¥ â”‚, â”œ, â•¯, â—‡ ç­‰å¼€å¤´çš„è¡Œï¼‰
            if line_stripped.startswith(('â”‚', 'â”œ', 'â•¯', 'â—‡')):
                continue

            # æ‰¾åˆ° JSON å¼€å§‹
            if line_stripped and (line_stripped.startswith('{') or line_stripped.startswith('"') or line_stripped.startswith('[')):
                json_start = i
                break

            # å¤„ç†å¸ƒå°”å€¼
            if line_stripped == "true":
                return {"ok": True, "result": True}
            if line_stripped == "false":
                return {"ok": True, "result": False}

            # å¤„ç†æ•°å­—ï¼ˆæ–°å¢ï¼‰
            if line_stripped.lstrip('-').isdigit():
                return {"ok": True, "result": int(line_stripped)}

        if json_start is not None:
            # ä» JSON å¼€å§‹è¡Œåˆå¹¶åç»­è¡Œç›´åˆ°æ‰¾åˆ°å®Œæ•´çš„ JSON
            json_text = lines[json_start]

            for j in range(json_start + 1, len(lines)):
                json_text += '\n' + lines[j]
                try:
                    response = json.loads(json_text)
                    if isinstance(response, str):
                        return {"ok": True, "result": response}
                    elif isinstance(response, dict):
                        if response.get("ok"):
                            return response
                        else:
                            return {"ok": True, "result": response.get("result")}
                    else:
                        return {"ok": True, "result": response}
                except json.JSONDecodeError:
                    # JSON è¿˜ä¸å®Œæ•´ï¼Œç»§ç»­åˆå¹¶
                    continue

            # å¾ªç¯ç»“æŸåï¼Œå†æ¬¡å°è¯•è§£æï¼ˆå¤„ç†å•è¡Œ JSON çš„æƒ…å†µï¼‰
            try:
                response = json.loads(json_text)
                if isinstance(response, str):
                    return {"ok": True, "result": response}
                elif isinstance(response, dict):
                    if response.get("ok"):
                        return response
                    else:
                        return {"ok": True, "result": response.get("result")}
                else:
                    return {"ok": True, "result": response}
            except json.JSONDecodeError:
                # åˆå¹¶äº†æ‰€æœ‰è¡Œè¿˜æ˜¯æ— æ³•è§£æ
                self.logger.warning(f"Failed to parse JSON: {json_text[:100]}")

        # æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè¾“å‡º
        self.logger.debug(f"Cannot find valid output. Lines: {lines[:10]}")  # æ˜¾ç¤ºå‰ 10 è¡Œç”¨äºè°ƒè¯•
        return None

    def _check_tab_not_found(self, result: subprocess.CompletedProcess) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ tab not found é”™è¯¯"""
        output = result.stderr + result.stdout
        return "tab not found" in output.lower()

    def _execute_action(self, action: str, **kwargs) -> Optional[Dict]:
        """æ‰§è¡Œæµè§ˆå™¨æ“ä½œï¼ˆå¸¦é‡è¯•å’Œè‡ªåŠ¨æ¢å¤ï¼‰"""
        for attempt in range(1, self.config.browser_action_max_attempts + 1):
            try:
                result = self._run_command(kwargs['cmd'], timeout=kwargs.get('timeout', 30))

                # æ£€æŸ¥æ˜¯å¦éœ€è¦æµè§ˆå™¨æ¢å¤
                if self._check_tab_not_found(result):
                    if attempt < self.config.browser_action_max_attempts:
                        self.logger.warning("Detected 'tab not found', attempting recovery...")
                        if self._restart_browser():
                            continue
                        else:
                            return None

                # æ£€æŸ¥è¿”å›ç 
                if result.returncode == 0:
                    return self._parse_output(result.stdout)
                else:
                    self.logger.error(f"Command failed: {result.stderr}")
                    return None

            except subprocess.TimeoutExpired:
                self.logger.error(f"Command timed out")
                return None
            except Exception as e:
                self.logger.error(f"Browser action error: {str(e)}")
                return None

        return None

    def _restart_browser(self) -> bool:
        """é‡å¯æµè§ˆå™¨æœåŠ¡"""
        if self.restart_count >= self.config.browser_max_restarts:
            self.logger.warning(f"Already restarted {self.restart_count} times, giving up")
            return False

        self.restart_count += 1
        self.logger.info(f"Attempting browser restart ({self.restart_count}/{self.config.browser_max_restarts})...")

        try:
            # åœæ­¢
            self.logger.info("Stopping browser...")
            self._run_command(["openclaw", "browser", "stop"])
            time.sleep(2)

            # å¯åŠ¨
            self.logger.info("Starting browser...")
            self._run_command(["openclaw", "browser", "start"])
            time.sleep(self.config.browser_restart_backoff)

            # éªŒè¯
            if self.check_status():
                self.logger.info("âœ… Browser restarted successfully")
                return True
            else:
                self.logger.error("âŒ Browser check failed")
                return False
        except Exception as e:
            self.logger.error(f"Browser restart failed: {str(e)}")
            return False

    def check_status(self) -> bool:
        """æ£€æŸ¥æµè§ˆå™¨çŠ¶æ€"""
        try:
            result = self._run_command(["openclaw", "browser", "status"], timeout=10)
            return result.returncode == 0 and "enabled: true" in result.stdout
        except Exception:
            return False

    def ensure_available(self) -> bool:
        """ç¡®ä¿æµè§ˆå™¨å¯ç”¨"""
        if self.check_status():
            return True

        self.logger.warning("Browser not available, attempting to fix...")
        try:
            self._run_command(["openclaw", "browser", "start"])
            time.sleep(3)
            return self.check_status()
        except Exception:
            return False

    def navigate(self, url: str) -> bool:
        """å¯¼èˆªåˆ° URL"""
        self.logger.info(f"Navigating to {url}")
        result = self._execute_action(
            action="navigate",
            cmd=["openclaw", "browser", "navigate", "--json", url]
        )
        if result:
            self.target_id = result.get("targetId")
            return True
        return False

    def evaluate(self, js_code: str) -> Optional[Any]:
        """æ‰§è¡Œ JavaScript"""
        if not self.target_id:
            self.logger.warning("No targetId available, cannot evaluate")
            return None

        result = self._execute_action(
            action="evaluate",
            cmd=["openclaw", "browser", "evaluate", "--target-id", self.target_id, "--fn", js_code]
        )

        if not result:
            return None

        result_value = result.get("result")
        self.logger.debug(f"Evaluate result type: {type(result_value)}")
        self.logger.debug(f"Evaluate result (repr): {repr(result_value)[:200]}")

        # ä¿®å¤ï¼šopenclaw browser evaluate è¿”å›çš„ JSON è¢«åŒé‡è½¬ä¹‰
        # _parse_output å¯èƒ½å·²ç»è§£æäº†ä¸€æ¬¡ï¼Œå¦‚æœç»“æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥è¿”å›
        if isinstance(result_value, list):
            self.logger.debug(f"Result is already a list: {len(result_value)} items")
            return result_value

        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ä¸”ä»¥ [ æˆ– { å¼€å¤´ï¼Œå°è¯•è§£æ
        if isinstance(result_value, str) and result_value.strip().startswith(('[', '{')):
            try:
                result_value = json.loads(result_value)
                self.logger.debug(f"Parsed evaluate result: string -> {type(result_value)}")
                # è§£æåå¯èƒ½è¿˜æ˜¯å­—ç¬¦ä¸²ï¼ˆåŒé‡è½¬ä¹‰ï¼‰
                if isinstance(result_value, str) and result_value.strip().startswith(('[', '{')):
                    result_value = json.loads(result_value)
                    self.logger.debug(f"Double-parsed evaluate result: string -> {type(result_value)}")
            except json.JSONDecodeError as e:
                self.logger.warning(f"Failed to parse evaluate result: {e}")
                return None

        return result_value

    def press(self, key: str) -> bool:
        """æŒ‰é”®"""
        result = self._execute_action(
            action="press",
            cmd=["openclaw", "browser", "press", key]
        )
        return result is not None

    def wait_for_content_loaded(self, timeout: int) -> bool:
        """æ™ºèƒ½ç­‰å¾…ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦çœŸçš„åŠ è½½å®Œæˆ"""
        js_code = "document.querySelectorAll('article').length > 0"

        start_time = time.time()
        while time.time() - start_time < timeout:
            result = self.evaluate(js_code)
            if result is True:
                self.logger.debug("Content loaded successfully")
                return True
            time.sleep(self.config.scroll_check_interval)

        self.logger.warning(f"Content not fully loaded after {timeout}s, proceeding anyway")
        return False


# ============================================================================
# Twitter API æ¨¡å—
# ============================================================================

class TwitterApiClient:
    """Twitter API å®¢æˆ·ç«¯ï¼ˆfxtwitter + syndicationï¼‰"""

    def __init__(self, config: Config, logger: Logger):
        self.config = config
        self.logger = logger

    def _fetch_via_fxtwitter(self, url: str) -> Optional[Dict]:
        """é€šè¿‡ fxtwitter API è·å–å†…å®¹"""
        api_url = re.sub(r'(x\.com|twitter\.com)', 'api.fxtwitter.com', url)
        try:
            resp = requests.get(api_url, headers=self.config.request_headers, timeout=15)
            if resp.status_code == 200:
                return resp.json()
            self.logger.warning(f"fxtwitter API returned {resp.status_code}")
        except Exception as e:
            self.logger.warning(f"fxtwitter error: {str(e)}")
        return None

    def _fetch_via_syndication(self, tweet_id: str) -> Optional[Dict]:
        """é€šè¿‡ syndication API è·å–å†…å®¹"""
        url = f"https://cdn.syndication.twimg.com/tweet-result?id={tweet_id}&token=0"
        try:
            resp = requests.get(url, headers=self.config.request_headers, timeout=10)
            if resp.status_code == 200:
                return resp.json()
            self.logger.warning(f"syndication API returned {resp.status_code}")
        except Exception as e:
            self.logger.warning(f"syndication error: {str(e)}")
        return None

    def fetch_post(self, url: str) -> Optional[Dict]:
        """è·å–å¸–å­å†…å®¹ï¼ˆå°è¯•å¤šä¸ª APIï¼‰"""
        tweet_id = self._extract_tweet_id(url)
        if not tweet_id:
            self.logger.warning(f"Cannot extract tweet ID from URL: {url}")
            return None

        # æ–¹æ³•1: fxtwitter
        data = self._fetch_via_fxtwitter(url)
        if data and data.get("tweet"):
            self.logger.debug(f"Successfully fetched via fxtwitter: {url}")
            return {"data": data, "source": "fxtwitter"}

        # æ–¹æ³•2: syndication
        data = self._fetch_via_syndication(tweet_id)
        if data and data.get("text"):
            self.logger.debug(f"Successfully fetched via syndication: {url}")
            return {"data": data, "source": "syndication"}

        self.logger.warning(f"Failed to fetch content: {url}")
        return None

    @staticmethod
    def _extract_tweet_id(url: str) -> Optional[str]:
        """ä» URL æå– tweet ID"""
        patterns = [
            r'(?:x\.com|twitter\.com)/\w+/status/(\d+)',
            r'(?:x\.com|twitter\.com)/\w+/statuses/(\d+)',
        ]
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None


# ============================================================================
# å¸–å­æ ¼å¼åŒ–æ¨¡å—
# ============================================================================

class PostFormatter:
    """å¸–å­å†…å®¹æ ¼å¼åŒ–å™¨"""

    def __init__(self, config: Config, logger: Logger):
        self.config = config
        self.logger = logger

    def format_as_markdown(self, post_data: Dict, source: str, url: str) -> str:
        """å°†å¸–å­æ•°æ®æ ¼å¼åŒ–ä¸º Markdown"""
        if source == "fxtwitter":
            return self._format_fxtwitter(post_data, url)
        elif source == "syndication":
            return self._format_syndication(post_data, url)
        return ""

    def _format_fxtwitter(self, data: Dict, url: str) -> str:
        """æ ¼å¼åŒ– fxtwitter æ•°æ®"""
        tweet = data.get("tweet", {})
        article = tweet.get("article")

        if article:
            return self._format_article(tweet, article, url)
        else:
            return self._format_tweet(tweet, url)

    def _format_syndication(self, data: Dict, url: str) -> str:
        """æ ¼å¼åŒ– syndication æ•°æ®"""
        user = data.get("user", {})
        lines = [
            f"# @{user.get('screen_name', '')} çš„æ¨æ–‡",
            "",
            f"> ä½œè€…: **{user.get('name', '')}** (@{user.get('screen_name', '')})",
            f"> å‘å¸ƒæ—¶é—´: {data.get('created_at', '')}",
            f"> åŸæ–‡é“¾æ¥: {url}",
            "",
            "---",
            "",
            data.get("text", ""),
            "",
        ]

        # åª’ä½“
        media_urls = [m.get("media_url_https") for m in data.get("mediaDetails", []) if m.get("media_url_https")]
        if media_urls:
            lines.extend(["## åª’ä½“", ""])
            lines.extend([f"![åª’ä½“{i}]({m})" for i, m in enumerate(media_urls, 1)])
            lines.append("")

        lines.extend([
            "---",
            "",
            "## äº’åŠ¨æ•°æ®",
            "",
            f"- â¤ï¸ ç‚¹èµ: {data.get('favorite_count', 0):,}",
            f"- ğŸ” è½¬å‘: {data.get('retweet_count', 0):,}",
        ])

        return "\n".join(lines)

    def _format_article(self, tweet: Dict, article: Dict, url: str) -> str:
        """æ ¼å¼åŒ– X Article"""
        lines = [
            f"# {article.get('title', 'Untitled')}",
            "",
            f"> ä½œè€…: **{tweet.get('author', {}).get('name', '')}** (@{tweet.get('author', {}).get('screen_name', '')})",
            f"> å‘å¸ƒæ—¶é—´: {article.get('created_at', '')}",
        ]

        if article.get('modified_at'):
            lines.append(f"> ä¿®æ”¹æ—¶é—´: {article.get('modified_at', '')}")

        lines.extend([
            f"> åŸæ–‡é“¾æ¥: {url}",
            "",
            "---",
            "",
        ])

        # å°é¢å›¾
        if article.get("cover_image"):
            lines.extend([
                f"![å°é¢]({article.get('cover_image')})",
                "",
            ])

        # æ­£æ–‡
        full_text = self._extract_article_content(article)
        if full_text:
            lines.extend([
                full_text,
                "",
            ])

        lines.extend([
            "---",
            "",
            "## äº’åŠ¨æ•°æ®",
            "",
            f"- â¤ï¸ ç‚¹èµ: {tweet.get('likes', 0):,}",
            f"- ğŸ” è½¬å‘: {tweet.get('retweets', 0):,}",
            f"- ğŸ‘€ æµè§ˆ: {tweet.get('views', 0):,}",
            f"- ğŸ”– ä¹¦ç­¾: {tweet.get('bookmarks', 0):,}",
        ])

        return "\n".join(lines)

    def _format_tweet(self, tweet: Dict, url: str) -> str:
        """æ ¼å¼åŒ–æ™®é€šæ¨æ–‡"""
        lines = [
            f"# @{tweet.get('author', {}).get('screen_name', '')} çš„æ¨æ–‡",
            "",
            f"> ä½œè€…: **{tweet.get('author', {}).get('name', '')}** (@{tweet.get('author', {}).get('screen_name', '')})",
            f"> å‘å¸ƒæ—¶é—´: {tweet.get('created_at', '')}",
            f"> åŸæ–‡é“¾æ¥: {url}",
            "",
            "---",
            "",
            tweet.get("text", ""),
            "",
        ]

        # åª’ä½“
        media_urls = [m.get("url") for m in tweet.get("media", {}).get("all", []) if m.get("url")]
        if media_urls:
            lines.extend(["## åª’ä½“", ""])
            lines.extend([f"![åª’ä½“{i}]({m})" for i, m in enumerate(media_urls, 1)])
            lines.append("")

        lines.extend([
            "---",
            "",
            "## äº’åŠ¨æ•°æ®",
            "",
            f"- â¤ï¸ ç‚¹èµ: {tweet.get('likes', 0):,}",
            f"- ğŸ” è½¬å‘: {tweet.get('retweets', 0):,}",
            f"- ğŸ‘€ æµè§ˆ: {tweet.get('views', 0):,}",
            f"- ğŸ’¬ å›å¤: {tweet.get('replies', 0):,}",
        ])

        return "\n".join(lines)

    def _extract_article_content(self, article: Dict) -> Optional[str]:
        """ä» X Article ä¸­æå–å®Œæ•´å†…å®¹"""
        if not article:
            return None

        content_blocks = article.get("content", {}).get("blocks", [])
        paragraphs = []

        for block in content_blocks:
            text = block.get("text", "").strip()
            block_type = block.get("type", "unstyled")

            if not text:
                continue

            type_to_format = {
                "header-one": f"# {text}",
                "header-two": f"## {text}",
                "header-three": f"### {text}",
                "blockquote": f"> {text}",
                "unordered-list-item": f"- {text}",
                "ordered-list-item": f"1. {text}",
            }

            paragraphs.append(type_to_format.get(block_type, text))

        return "\n\n".join(paragraphs)


# ============================================================================
# æŠ“å–æ¨¡å—
# ============================================================================

class PostCrawler:
    """å¸–å­æŠ“å–å™¨"""

    def __init__(self, config: Config, logger: Logger, browser: BrowserClient):
        self.config = config
        self.logger = logger
        self.browser = browser

    def _get_scroll_js(self) -> str:
        """è·å–æ»šåŠ¨çš„ JavaScript ä»£ç """
        # ä½¿ç”¨ IIFE ç«‹å³æ‰§è¡Œå¹¶è¿”å›ç»“æœï¼ˆç®­å¤´å‡½æ•°éšå¼è¿”å›ï¼‰
        return """(() => {
            const articles = document.querySelectorAll('article');
            const now = new Date();
            const oneDayAgo = new Date(now.getTime() - 24 * 60 * 60 * 1000);
            const result = [];

            for (let i = 0; i < articles.length; i++) {
                const article = articles[i];
                const timeElement = article.querySelector('time');
                if (!timeElement) continue;

                const datetime = timeElement.getAttribute('datetime');
                if (!datetime) continue;

                const tweetDate = new Date(datetime);
                if (tweetDate < oneDayAgo) continue;

                const links = article.querySelectorAll('a[href*="/status/"]');
                for (let j = 0; j < links.length; j++) {
                    const link = links[j];
                    const href = link.getAttribute('href');
                    if (href && href.includes('/status/')) {
                        const statusId = href.split('/status/')[1].split('/')[0];
                        const fullUrl = 'https://x.com' + href.split('/status/')[0] + '/status/' + statusId;
                        if (!result.includes(fullUrl)) {
                            result.push(fullUrl);
                        }
                        break;
                    }
                }
            }

            return result
        })()"""

    def crawl_user(self, username: str) -> List[str]:
        """æŠ“å–å•ä¸ªç”¨æˆ·çš„å¸–å­"""
        self.logger.info(f"Starting crawl for @{username}")

        if not self.browser.ensure_available():
            raise Exception("Browser not available")

        if not self.browser.navigate(f"https://x.com/{username}"):
            raise Exception(f"Failed to navigate to @{username}")

        # æ™ºèƒ½ç­‰å¾…é¡µé¢åŠ è½½
        if self.browser.wait_for_content_loaded(self.config.page_load_timeout):
            self.logger.debug("Page loaded, ready to scroll")
        else:
            time.sleep(random.uniform(1, 2))  # å¤‡ç”¨ç­‰å¾…

        # æ»šåŠ¨å¹¶æ”¶é›† URL
        urls = self._scroll_and_collect(username)

        if not urls:
            raise NoPostsFoundError(f"No posts found for @{username} in the last 24 hours")

        return urls

    def _scroll_and_collect(self, username: str) -> List[str]:
        """æ»šåŠ¨é¡µé¢å¹¶æ”¶é›† URLï¼ˆæ™ºèƒ½ç­–ç•¥ï¼‰"""
        urls = []
        seen_ids = set()
        no_new_count = 0
        consecutive_errors = 0
        found_yesterday = False  # æ£€æµ‹æ˜¯å¦æ‰¾åˆ°æ˜¨å¤©çš„å¸–å­

        for scroll_num in range(1, self.config.scroll_max_attempts + 1):
            self.logger.debug(f"Scroll {scroll_num}/{self.config.scroll_max_attempts}")

            # æ‰§è¡Œ JavaScript
            js_code = self._get_scroll_js()
            result = self.browser.evaluate(js_code)

            if result is not None:
                consecutive_errors = 0
                try:
                    # result å¯èƒ½å·²ç»æ˜¯åˆ—è¡¨ï¼Œæˆ–è€…éœ€è¦ json.loads è§£æ
                    if isinstance(result, list):
                        page_urls = result
                    else:
                        page_urls = json.loads(result)
                    new_urls = [u for u in page_urls if u not in seen_ids]

                    if new_urls:
                        self.logger.debug(f"Found {len(new_urls)} new URLs")
                        seen_ids.update(new_urls)
                        urls.extend(new_urls)
                        no_new_count = 0
                    else:
                        no_new_count += 1
                        self.logger.debug(f"No new posts (count: {no_new_count})")

                        # ä¼˜åŒ–ï¼šè¿ç»­ 2 æ¬¡æ— æ–°å¸–å°±åœæ­¢ï¼ˆåŸä¸º 3 æ¬¡ï¼‰
                        if no_new_count >= self.config.scroll_no_new_threshold:
                            self.logger.info("No new posts for 2 scrolls, stopping early")
                            break

                        # ä¼˜åŒ–ï¼šå¦‚æœé…ç½®äº†ï¼Œæ£€æµ‹åˆ°æ˜¨å¤©çš„å¸–å­å°±ç«‹å³åœæ­¢
                        # é€šè¿‡è§‚å¯Ÿç¬¬ä¸€ä¸ªå¸–å­çš„ URL ä¸­çš„æ—¶é—´æˆ³åˆ¤æ–­æ˜¯å¦æ˜¯æ˜¨å¤©çš„
                        if self.config.scroll_early_stop_on_yesterday and scroll_num == 1 and urls:
                            first_url = urls[0]
                            # ä» URL ä¸­æå– tweet IDï¼Œåˆ¤æ–­æ—¶é—´
                            # æ ¼å¼ï¼šhttps://x.com/username/status/1234567890
                            tweet_id = first_url.split('/status/')[-1]
                            if tweet_id:
                                tweet_timestamp = int(tweet_id)
                                # Twitter çš„ Snowflake ID ä¸­ï¼Œæ—¶é—´æˆ³æ˜¯å‰ 41 ä½
                                # å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ï¼‰è½¬æ¢ä¸ºç§’
                                import time as time_module
                                current_ts = time_module.time() * 1000
                                # è®¡ç®— 24 å°æ—¶å‰çš„æ—¶é—´æˆ³
                                one_day_ago = current_ts - (24 * 60 * 60 * 1000)
                                if tweet_timestamp < one_day_ago:
                                    self.logger.info("Detected yesterday's post, stopping immediately")
                                    found_yesterday = True
                                    break
                except json.JSONDecodeError as e:
                    self.logger.error(f"Failed to parse URLs: {str(e)}")
                    consecutive_errors += 1
            else:
                consecutive_errors += 1

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ”¾å¼ƒ
            if consecutive_errors >= self.config.scroll_max_consecutive_errors:
                self.logger.error(f"Too many consecutive errors, giving up on @{username}")
                break

            # ä¼˜åŒ–ï¼šå¦‚æœå·²ç»æ£€æµ‹åˆ°æ˜¨å¤©çš„å¸–å­ï¼Œç«‹å³åœæ­¢
            if found_yesterday:
                break

            # æ»šåŠ¨
            if scroll_num < self.config.scroll_max_attempts:
                self.browser.press("PageDown")
                time.sleep(random.uniform(
                    self.config.scroll_check_interval,
                    self.config.scroll_check_interval * 2
                ))

        self.logger.info(f"Collected {len(urls)} URLs for @{username}")
        return urls


# ============================================================================
# ç”¨æˆ·ç®¡ç†æ¨¡å—
# ============================================================================

class UserManager:
    """ç”¨æˆ·åˆ—è¡¨ç®¡ç†"""

    def __init__(self, config: Config, logger: Logger):
        self.config = config
        self.logger = logger

    def load(self) -> List[str]:
        """åŠ è½½ç”¨æˆ·åˆ—è¡¨"""
        if not self.config.users_file.exists():
            self.logger.warning(f"Users file not found: {self.config.users_file}")
            return []

        users = []
        with open(self.config.users_file, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    users.append(line)

        self.logger.info(f"Loaded {len(users)} users from {self.config.users_file}")
        return users

    def add(self, username: str) -> None:
        """æ·»åŠ ç”¨æˆ·"""
        users = self.load()
        if username in users:
            self.logger.warning(f"User already exists: {username}")
            return

        users.append(username)
        self._save(users)
        self.logger.info(f"Added user: {username}")

    def remove(self, username: str) -> None:
        """åˆ é™¤ç”¨æˆ·"""
        users = self.load()
        if username not in users:
            self.logger.warning(f"User not found: {username}")
            return

        users.remove(username)
        self._save(users)
        self.logger.info(f"Removed user: {username}")

    def list(self) -> None:
        """åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·"""
        users = self.load()
        self.logger.info(f"User list ({len(users)} users):")
        for i, user in enumerate(users, 1):
            self.logger.info(f"  {i}. {user}")

    def _save(self, users: List[str]) -> None:
        """ä¿å­˜ç”¨æˆ·åˆ—è¡¨"""
        with open(self.config.users_file, "w", encoding="utf-8") as f:
            for user in users:
                f.write(f"{user}\n")
        self.logger.info(f"Saved {len(users)} users")


# ============================================================================
# æ–‡ä»¶ç®¡ç†æ¨¡å—
# ============================================================================

class ResultFileManager:
    """ç»“æœæ–‡ä»¶ç®¡ç†"""

    def __init__(self, config: Config, logger: Logger):
        self.config = config
        self.logger = logger
        self.config.results_dir.mkdir(parents=True, exist_ok=True)

    def _get_filename(self, timestamp: datetime) -> tuple[Path, Path]:
        """ç”Ÿæˆæ–‡ä»¶å"""
        basename = f"posts_{timestamp.strftime('%Y%m%d_%H%M%S')}"
        return (
            self.config.results_dir / f"{basename}.txt",
            self.config.results_dir / f"{basename}.md"
        )

    def create_txt(self, timestamp: datetime, users: List[str]) -> Path:
        """åˆ›å»º TXT æ–‡ä»¶"""
        filepath, _ = self._get_filename(timestamp)

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"# Crawl Results - {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Total users: {len(users)}\n")
            f.write(f"# Started at: {timestamp.strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        self.logger.info(f"Created TXT file: {filepath}")
        return filepath

    def append_user_results(self, filepath: Path, username: str, urls: List[str],
                          current: int, total: int) -> None:
        """è¿½åŠ ç”¨æˆ·ç»“æœåˆ° TXT æ–‡ä»¶"""
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(f"# @{username} ({len(urls)} posts) - [{current}/{total}]\n")
            for url in urls:
                f.write(f"{url}\n")
            f.write("\n")
            f.flush()

    def finalize_txt(self, filepath: Path, total_posts: int) -> None:
        """å®Œæˆ TXT æ–‡ä»¶"""
        with open(filepath, "a", encoding="utf-8") as f:
            f.write(f"\n# Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# Total posts: {total_posts}\n")
            f.flush()

    def save_markdown(self, results: Dict[str, List[str]], timestamp: datetime,
                      api_client: TwitterApiClient, formatter: PostFormatter) -> Path:
        """ç”Ÿæˆ Markdown æ–‡ä»¶ï¼ˆå¹¶å‘è·å–å†…å®¹ï¼‰"""
        _, md_filepath = self._get_filename(timestamp)

        # å‡†å¤‡æ ‡é¢˜
        header = [
            "# X å¸–å­æŠ“å–ç»“æœ",
            f"\n**æŠ“å–æ—¶é—´:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**æ€»ç”¨æˆ·æ•°:** {len(results)}",
            f"**æ€»å¸–å­æ•°:** {sum(len(urls) for urls in results.values())}",
            "\n---\n"
        ]

        # æ”¶é›†æ‰€æœ‰å¸–å­ä¿¡æ¯
        post_infos = []
        content = list(header)

        for user, urls in results.items():
            if not urls:
                continue

            content.extend([f"\n## @{user} çš„å¸–å­ ({len(urls)} æ¡)\n", "---\n"])

            for i, url in enumerate(urls, 1):
                section_start = len(content)
                post_infos.append({'user': user, 'url': url, 'index': i, 'total': len(urls), 'section_start': section_start})

                # å ä½ç¬¦
                content.extend([
                    f"\n### å¸–å­ {i}\n\n",
                    "> æ­£åœ¨è·å–å†…å®¹...\n\n",
                    f"- URL: {url}\n\n",
                    "---\n\n"
                ])

        # å¹¶å‘è·å–å†…å®¹
        from concurrent.futures import ThreadPoolExecutor, as_completed

        self.logger.info(f"Fetching content for {len(post_infos)} posts concurrently...")

        def fetch_content(info):
            post_data = api_client.fetch_post(info['url'])
            if post_data:
                return info, formatter.format_as_markdown(
                    post_data['data'],
                    post_data['source'],
                    info['url']
                )
            return info, None

        with ThreadPoolExecutor(max_workers=self.config.content_fetch_workers) as executor:
            futures = {executor.submit(fetch_content, info): info for info in post_infos}

            completed = 0
            for future in as_completed(futures):
                completed += 1
                info = futures[future]

                try:
                    _, markdown = future.result()
                    if markdown:
                        content[info['section_start']] = markdown + "\n\n---\n\n"
                        self.logger.info(f"Fetched {info['user']} post {info['index']}/{info['total']} [{completed}/{len(post_infos)}]")
                    else:
                        content[info['section_start']] = (
                            f"\n### å¸–å­ {info['index']}\n\n"
                            "> âš ï¸ æ— æ³•è·å–å¸–å­å†…å®¹\n\n"
                            f"- URL: {info['url']}\n\n"
                            "---\n\n"
                        )
                except Exception as e:
                    self.logger.error(f"Error fetching {info['url']}: {str(e)}")

        # ä¿å­˜
        with open(md_filepath, "w", encoding="utf-8") as f:
            f.write("".join(content))

        self.logger.info(f"Markdown saved to {md_filepath}")
        return md_filepath


# ============================================================================
# ä¸»æ§åˆ¶å™¨
# ============================================================================

class CrawlHot:
    """Crawl Hot ä¸»æ§åˆ¶å™¨ï¼ˆåè°ƒæ‰€æœ‰æ¨¡å—ï¼‰"""

    def __init__(self):
        # åˆå§‹åŒ–é…ç½®
        self.config = Config()
        self.logger = Logger(self.config.log_file)

        # è¿›ç¨‹é”
        self.lock = ProcessLock(self.config.base_dir / ".craw_hot.lock")
        if not self.lock.acquire():
            print(f"âŒ Error: Another craw-hot instance is already running!")
            print(f"   Lock file: {self.lock.lock_file}")
            print(f"   Command: rm {self.lock.lock_file}")
            sys.exit(1)

        # åˆå§‹åŒ–å„ä¸ªæ¨¡å—
        self.browser = BrowserClient(self.config, self.logger)
        self.api_client = TwitterApiClient(self.config, self.logger)
        self.formatter = PostFormatter(self.config, self.logger)
        self.user_manager = UserManager(self.config, self.logger)
        self.result_manager = ResultFileManager(self.config, self.logger)

        # æµè§ˆå™¨æ“ä½œé”ï¼ˆç”¨äºå¹¶å‘æŠ“å–ï¼‰
        self.browser_lock = threading.Lock()

        self.logger.info("=" * 60)
        self.logger.info("CrawlHot initialized")

    def __del__(self):
        """ææ„å‡½æ•°"""
        if hasattr(self, 'lock'):
            self.lock.release()

    def _retry_with_timeout(self, func: Callable, username: str, timeout: int) -> List[str]:
        """å¸¦è¶…æ—¶å’Œé‡è¯•çš„æ‰§è¡Œå™¨"""
        import threading

        result = []
        exception = [None]

        def _worker():
            try:
                result.extend(func())
            except NoPostsFoundError:
                pass  # æ­£å¸¸æƒ…å†µï¼Œä¸è®°å½•
            except Exception as e:
                exception[0] = e

        def _timeout_handler():
            exception[0] = CrawlTimeoutError(f"Crawl for @{username} timed out after {timeout}s")

        thread = threading.Thread(target=_worker)
        timer = threading.Timer(timeout, _timeout_handler)

        try:
            thread.start()
            timer.start()
            thread.join(timeout=timeout + 5)
            timer.cancel()

            if exception[0]:
                self.logger.error(f"Exception: {str(exception[0])}")
                return []

            return result
        except Exception as e:
            self.logger.error(f"Unexpected error: {str(e)}")
            return []

    def crawl_single_user(self, username: str) -> List[str]:
        """æŠ“å–å•ä¸ªç”¨æˆ·ï¼ˆå¸¦æµè§ˆå™¨é”ä¿æŠ¤ï¼‰"""
        with self.browser_lock:
            return self._retry_with_timeout(
                lambda: self._retry_crawl_user(username),
                username=username,
                timeout=self.config.user_crawl_timeout
            )

    def _retry_crawl_user(self, username: str) -> List[str]:
        """å¸¦é‡è¯•çš„æŠ“å–"""
        for attempt in range(1, self.config.max_retries + 1):
            try:
                self.logger.debug(f"Attempt {attempt}/{self.config.max_retries}")
                crawler = PostCrawler(self.config, self.logger, self.browser)
                return crawler.crawl_user(username)
            except NoPostsFoundError:
                self.logger.info(f"@{username}: no posts found in the last 24 hours")
                return []
            except Exception as e:
                self.logger.error(f"Attempt {attempt} failed: {str(e)}")

                if attempt < self.config.max_retries:
                    wait_time = random.uniform(1, 3) * attempt
                    self.logger.warning(f"Retrying in {wait_time:.2f}s...")
                    time.sleep(wait_time)

        self.logger.error(f"All attempts failed for @{username}")
        return []

    def crawl_all_users(self) -> Dict[str, List[str]]:
        """æŠ“å–æ‰€æœ‰ç”¨æˆ·ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰"""
        users = self.user_manager.load()
        if not users:
            self.logger.warning("No users to crawl")
            return {}

        timestamp = datetime.now()
        all_results = {}

        # åˆ›å»ºç»“æœæ–‡ä»¶
        txt_filepath = self.result_manager.create_txt(timestamp, users)

        # å¹¶å‘æŠ“å–
        self.logger.info(f"Starting crawl (concurrent mode, {self.config.user_crawl_workers} workers)...")

        completed_count = 0
        with ThreadPoolExecutor(max_workers=self.config.user_crawl_workers) as executor:
            # æäº¤æ‰€æœ‰ç”¨æˆ·çš„æŠ“å–ä»»åŠ¡
            future_to_user = {
                executor.submit(self.crawl_single_user, user): user
                for user in users
            }

            # æŒ‰å®Œæˆé¡ºåºå¤„ç†ç»“æœ
            for future in as_completed(future_to_user):
                user = future_to_user[future]
                completed_count += 1

                try:
                    urls = future.result()
                    all_results[user] = urls

                    # ç«‹å³ä¿å­˜
                    self.result_manager.append_user_results(txt_filepath, user, urls, completed_count, len(users))

                    if urls:
                        self.logger.info(f"@{user}: {len(urls)} posts [{completed_count}/{len(users)}]")
                    else:
                        self.logger.warning(f"@{user}: no posts found [{completed_count}/{len(users)}]")
                except Exception as e:
                    self.logger.error(f"Failed to crawl @{user}: {str(e)}")
                    all_results[user] = []

                    # å³ä½¿å¤±è´¥ä¹Ÿè¿½åŠ ç©ºç»“æœ
                    self.result_manager.append_user_results(txt_filepath, user, [], completed_count, len(users))

        # å®Œæˆ TXT æ–‡ä»¶
        total_posts = sum(len(urls) for urls in all_results.values())
        self.result_manager.finalize_txt(txt_filepath, total_posts)

        # ç”Ÿæˆ Markdown æ–‡ä»¶
        self.result_manager.save_markdown(
            all_results,
            timestamp,
            self.api_client,
            self.formatter
        )

        self._print_summary(all_results)
        return all_results

    def _print_summary(self, results: Dict[str, List[str]]) -> None:
        """æ‰“å°æ‘˜è¦"""
        total_posts = sum(len(urls) for urls in results.values())
        self.logger.info("=" * 60)
        self.logger.info("CRAWL SUMMARY")
        self.logger.info("=" * 60)
        self.logger.info(f"Total users: {len(results)}")
        self.logger.info(f"Total posts: {total_posts}")
        self.logger.info("")
        for user, urls in results.items():
            self.logger.info(f"@{user}: {len(urls)} posts")
        self.logger.info("=" * 60)


# ============================================================================
# CLI å…¥å£
# ============================================================================

def main():
    """CLI å…¥å£"""
    crawler = None

    try:
        crawler = CrawlHot()

        if len(sys.argv) < 2:
            print("Usage: python craw_hot_refactored.py <command> [args]")
            print("\nCommands:")
            print("  add <username>     - Add a user to list")
            print("  remove <username>  - Remove a user from list")
            print("  list               - List all users")
            print("  crawl              - Crawl all users' today posts")
            print("  crawl <username>   - Crawl a single user's today posts")
            sys.exit(1)

        command = sys.argv[1].lower()

        if command == "add":
            if len(sys.argv) < 3:
                print("Error: username required")
                sys.exit(1)
            crawler.user_manager.add(sys.argv[2])

        elif command == "remove":
            if len(sys.argv) < 3:
                print("Error: username required")
                sys.exit(1)
            crawler.user_manager.remove(sys.argv[2])

        elif command == "list":
            crawler.user_manager.list()

        elif command == "crawl":
            if len(sys.argv) >= 3:
                username = sys.argv[2]
                urls = crawler.crawl_single_user(username)
                if urls:
                    print(f"\n@{username} found {len(urls)} posts:")
                    for url in urls:
                        print(f"  {url}")
                else:
                    print(f"\n@{username}: no posts found today")
            else:
                results = crawler.crawl_all_users()

        else:
            print(f"Unknown command: {command}")
            sys.exit(1)

    finally:
        if crawler:
            crawler.lock.release()


if __name__ == "__main__":
    main()
